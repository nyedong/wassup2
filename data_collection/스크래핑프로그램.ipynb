{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d53302a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import math, time\n",
    "import time, random, pandas as pd\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException,StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time, urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "93449ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어:전주\n",
      "스크래핑 할 건수는 몇건입니까?: 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "options = Options()\n",
    "options.add_argument('--window-size=782,824') \n",
    "options.add_argument('--window-position=-8,0')\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "search = input('검색어:')\n",
    "cnt = int(input('스크래핑 할 건수는 몇건입니까?: '))\n",
    "page_cnt = math.ceil(cnt / 10)  # 크롤링 할 전체 페이지 수\n",
    "\n",
    "now = time.localtime()\n",
    "date_format = '%04d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday)\n",
    "f_dir = f'{os.getcwd()}\\\\{search}여행기사_{cnt}건_{date_format}'\n",
    "os.makedirs(f_dir)\n",
    "\n",
    "URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c59027f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여행기사 더보기 클릭\n",
    "# driver.find_element(By.CSS_SELECTOR, \".more_view\").click() 중복주의\n",
    "driver.find_element(By.CSS_SELECTOR, \"#s_recommend > .more_view > a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af439513",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "contents_list = []\n",
    "img_url_list = []\n",
    "\n",
    "def page_work():\n",
    "    result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')\n",
    "    global contents_no, cnt\n",
    "    global title_list, contents_list, img_url_list\n",
    "    \n",
    "    for item in result:\n",
    "        contents_no += 1\n",
    "        \n",
    "        if contents_no <= cnt :    \n",
    "            print(f'[콘텐츠 {contents_no}]')  \n",
    "            item.send_keys(Keys.ENTER) # .click()은 에러 잘남\n",
    "\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 이미지 추출을 위해 미리 스크롤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            html = driver.page_source\n",
    "            html_dom = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            title = html_dom.find(id='topTitle')\n",
    "            title_list.append(title.text)\n",
    "            print(title.text)\n",
    "            \n",
    "            img_tag_list = html_dom.select('.img_typeBox img')\n",
    "            img_url_list = [item['src'] for item in img_tag_list]\n",
    "\n",
    "            contents = driver.find_elements(By.CLASS_NAME, 'txt_p')\n",
    "            contents_merge = ' '.join([item.text for item in contents])        \n",
    "            contents_list.append(contents_merge)           \n",
    "            \n",
    "            driver.back()\n",
    "            time.sleep(2)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e4152b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_export():\n",
    "\n",
    "    DF = pd.DataFrame({\"제목\":title_list, \"내용\":contents_list})\n",
    "    filename = f'{search}여행기사_{cnt}건_{date_format}.xlsx'\n",
    "    DF.to_excel(f_dir+'\\\\'+filename)\n",
    "    print(f'====== {page_no} 페이지 {filename} 파일 저장 완료 ======')\n",
    "\n",
    "\n",
    "    no = 0\n",
    "    for src in img_url_list:\n",
    "        # 다운로드  (주소, 파일이름)\n",
    "        urllib.request.urlretrieve(src, f'{f_dir}\\\\{page_no}_{no}.jpg')\n",
    "        no += 1\n",
    "    print(f'====== {page_no} 페이지 {f_dir} 디렉토리 이미지 저장 완료 ======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bd036cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑 프로그램 실행\n",
      "====== 1 페이지 스크래핑 시작 ======\n",
      "[콘텐츠 1]\n",
      "낭만 가득한 여행 천국, 전주\n",
      "[콘텐츠 2]\n",
      "전주 여행, 기차 타고 떠나기 좋은 가을 낭만 여행\n",
      "[콘텐츠 3]\n",
      "[살아보기 여행-전주편] ‘반반(半半) 전주’ \n",
      "[콘텐츠 4]\n",
      "비빔밥 와플부터 콩나물 아이스크림까지! 요즘 뜨는 전주의 맛 6\n",
      "[콘텐츠 5]\n",
      "풋풋한 청춘 서사 〈스물다섯 스물하나〉 속 전주 서학동예술마을과 한벽굴\n",
      "[콘텐츠 6]\n",
      "[전주] 천천히, 오래 지켜가는 지속 가능한 전주, 사랑가득 한옥 펜션\n",
      "[콘텐츠 7]\n",
      "[전주] 슬로시티 전주에서 바삐 달려왔던 일상에 쉼표를, 전주 정가한옥\n",
      "[콘텐츠 8]\n",
      "가장 한국전인 멋의 고장, 전주한옥마을\n",
      "[콘텐츠 9]\n",
      "전주여행, 색다르게 즐기는 전주 핫플레이스\n",
      "[콘텐츠 10]\n",
      " [완(주)전(주)익(산)사이팅한 역사 길] 과거와 현대가 공존하는 곳, 전주 전주천길\n",
      "====== 1 페이지 스크래핑 완료 ======\n",
      "====== 1 페이지 전주여행기사_13건_2024320.xlsx파일 저장 완료 ======\n",
      "====== 2 페이지 스크래핑 시작 ======\n",
      "[콘텐츠 11]\n",
      "낭만 가득한 여행 천국, 전주\n",
      "[콘텐츠 12]\n",
      "전주 여행, 기차 타고 떠나기 좋은 가을 낭만 여행\n",
      "[콘텐츠 13]\n",
      "[살아보기 여행-전주편] ‘반반(半半) 전주’ \n",
      "====== 2 페이지 스크래핑 완료 ======\n",
      "====== 2 페이지 전주여행기사_13건_2024320.xlsx파일 저장 완료 ======\n",
      "스크래핑 프로그램 종료\n"
     ]
    }
   ],
   "source": [
    "contents_no = 0\n",
    "today = time.localtime()\n",
    "print('스크래핑 프로그램 실행')\n",
    "\n",
    "for page_no in range(1, page_cnt+1):\n",
    "    print(f'====== {page_no} 페이지 스크래핑 시작 ======')\n",
    "    page_work()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 완료 ======')\n",
    "\n",
    "    DF = pd.DataFrame({\"제목\":title_list, \"내용\":contents_list})\n",
    "    filename = f'{search}여행기사_{cnt}건_{today.tm_year}{today.tm_mon}{today.tm_mday}.xlsx'\n",
    "    DF.to_excel(filename)\n",
    "    print(f'====== {page_no} 페이지 {filename}파일 저장 완료 ======')\n",
    "\n",
    "    if page_no < page_cnt:\n",
    "        next = driver.find_element(By.CSS_SELECTOR, f'a[id=\"{page_no+1}\"]')\n",
    "        next.click\n",
    "        \n",
    "    # try    \n",
    "            \n",
    "    #except StaleElementReferenceException:\n",
    "     #      clickUnderTen.click()\n",
    "            \n",
    "            \n",
    "       # except ElementClickInterceptedException:\n",
    "        # driver.find_elements_by_css_selector(\"[data-testid='loadingButton']\")[0].sendkeys(Keys.ENTER)\n",
    "        #    clickUnderTen.click()\n",
    "        \n",
    "print('스크래핑 프로그램 종료')\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
